{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f13433b",
   "metadata": {},
   "source": [
    "## Tanishq Keswani\n",
    "### Practical 4\n",
    "\n",
    "#### AIM - Perform morphological analysis and lemmatization on the text corpora. Detect and Extract specific entities from the text using NER(Named Entity Recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e9954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d2db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21a331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ef6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import punkt\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea550db",
   "metadata": {},
   "source": [
    "### 4(a) Write a Python NLTK program to perform Stemming and Lemmatization on set of tokens. PorterStemmer(), SnowballStemmer(), LancasterStemmer(), RegexpStemmer() WordNetLemmatizer() -lemmatizer.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d70703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Noah Chomsky, one  of the first linguists of twelfth century that started syntactic theories, marked a unique position in the field of theoretical linguistics because he revolutionised the area of syntax (Chomsky, 1965) . Which can be broadly categorized into two levels Higher Level  which  include  speech  recognition  and  Lower  Level  which  corresponds  to  natural language. Few of the researched tasks of NLP are Automatic Summarization, Co-Reference Resolution, Discourse Analysis, Machine Translation, Morphological Segmentation, Named Entity Recognition,  Optical Character  Recognition, Part  Of Speech  Tagging etc. Some  of these tasks have  direct real  world applications  such as  Machine translation,  Named entity recognition,  Optical  character  recognition  etc.  Automatic  summarization  produces  an understandable summary of a set of text and provides summaries or detailed information of text of a known type. Co-reference resolution it refers to a sentence or larger set of text that determines which  word refer  to  the same  object. Discourse  analysis refers  to the  task of identifying the  discourse structure of  connected text.  Machine translation  which refers  to automatic  translation  of  text  from  one  human  language  to  another.  Morphological segmentation which refers to separate word into individual morphemes and identify the class of the morphemes. Named entity recognition (NER) it describes a stream of text, determine which items in the text relates to proper names. Optical character recognition (OCR) it gives an image representing printed text, which help in determining the corresponding or related text. Part of speech tagging it describes a sentence, determines the part of speech for each word.  Though  NLP  tasks  are  obviously  very  closely  interweaved  but  they  are  used frequently, for convenience. Some of the task such as automatic summarisation, co-reference analysis etc. act as subtask that are used in solving larger tasks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d8edb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "token1= word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883a6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Noah',\n",
       " 'Chomsky',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'linguists',\n",
       " 'of',\n",
       " 'twelfth',\n",
       " 'century',\n",
       " 'that',\n",
       " 'started',\n",
       " 'syntactic',\n",
       " 'theories',\n",
       " ',',\n",
       " 'marked',\n",
       " 'a',\n",
       " 'unique',\n",
       " 'position',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'of',\n",
       " 'theoretical',\n",
       " 'linguistics',\n",
       " 'because',\n",
       " 'he',\n",
       " 'revolutionised',\n",
       " 'the',\n",
       " 'area',\n",
       " 'of',\n",
       " 'syntax',\n",
       " '(',\n",
       " 'Chomsky',\n",
       " ',',\n",
       " '1965',\n",
       " ')',\n",
       " '.',\n",
       " 'Which',\n",
       " 'can',\n",
       " 'be',\n",
       " 'broadly',\n",
       " 'categorized',\n",
       " 'into',\n",
       " 'two',\n",
       " 'levels',\n",
       " 'Higher',\n",
       " 'Level',\n",
       " 'which',\n",
       " 'include',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'and',\n",
       " 'Lower',\n",
       " 'Level',\n",
       " 'which',\n",
       " 'corresponds',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " '.',\n",
       " 'Few',\n",
       " 'of',\n",
       " 'the',\n",
       " 'researched',\n",
       " 'tasks',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'are',\n",
       " 'Automatic',\n",
       " 'Summarization',\n",
       " ',',\n",
       " 'Co-Reference',\n",
       " 'Resolution',\n",
       " ',',\n",
       " 'Discourse',\n",
       " 'Analysis',\n",
       " ',',\n",
       " 'Machine',\n",
       " 'Translation',\n",
       " ',',\n",
       " 'Morphological',\n",
       " 'Segmentation',\n",
       " ',',\n",
       " 'Named',\n",
       " 'Entity',\n",
       " 'Recognition',\n",
       " ',',\n",
       " 'Optical',\n",
       " 'Character',\n",
       " 'Recognition',\n",
       " ',',\n",
       " 'Part',\n",
       " 'Of',\n",
       " 'Speech',\n",
       " 'Tagging',\n",
       " 'etc',\n",
       " '.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'these',\n",
       " 'tasks',\n",
       " 'have',\n",
       " 'direct',\n",
       " 'real',\n",
       " 'world',\n",
       " 'applications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Machine',\n",
       " 'translation',\n",
       " ',',\n",
       " 'Named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'Optical',\n",
       " 'character',\n",
       " 'recognition',\n",
       " 'etc',\n",
       " '.',\n",
       " 'Automatic',\n",
       " 'summarization',\n",
       " 'produces',\n",
       " 'an',\n",
       " 'understandable',\n",
       " 'summary',\n",
       " 'of',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'text',\n",
       " 'and',\n",
       " 'provides',\n",
       " 'summaries',\n",
       " 'or',\n",
       " 'detailed',\n",
       " 'information',\n",
       " 'of',\n",
       " 'text',\n",
       " 'of',\n",
       " 'a',\n",
       " 'known',\n",
       " 'type',\n",
       " '.',\n",
       " 'Co-reference',\n",
       " 'resolution',\n",
       " 'it',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'or',\n",
       " 'larger',\n",
       " 'set',\n",
       " 'of',\n",
       " 'text',\n",
       " 'that',\n",
       " 'determines',\n",
       " 'which',\n",
       " 'word',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'same',\n",
       " 'object',\n",
       " '.',\n",
       " 'Discourse',\n",
       " 'analysis',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'identifying',\n",
       " 'the',\n",
       " 'discourse',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'connected',\n",
       " 'text',\n",
       " '.',\n",
       " 'Machine',\n",
       " 'translation',\n",
       " 'which',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'automatic',\n",
       " 'translation',\n",
       " 'of',\n",
       " 'text',\n",
       " 'from',\n",
       " 'one',\n",
       " 'human',\n",
       " 'language',\n",
       " 'to',\n",
       " 'another',\n",
       " '.',\n",
       " 'Morphological',\n",
       " 'segmentation',\n",
       " 'which',\n",
       " 'refers',\n",
       " 'to',\n",
       " 'separate',\n",
       " 'word',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'morphemes',\n",
       " 'and',\n",
       " 'identify',\n",
       " 'the',\n",
       " 'class',\n",
       " 'of',\n",
       " 'the',\n",
       " 'morphemes',\n",
       " '.',\n",
       " 'Named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " '(',\n",
       " 'NER',\n",
       " ')',\n",
       " 'it',\n",
       " 'describes',\n",
       " 'a',\n",
       " 'stream',\n",
       " 'of',\n",
       " 'text',\n",
       " ',',\n",
       " 'determine',\n",
       " 'which',\n",
       " 'items',\n",
       " 'in',\n",
       " 'the',\n",
       " 'text',\n",
       " 'relates',\n",
       " 'to',\n",
       " 'proper',\n",
       " 'names',\n",
       " '.',\n",
       " 'Optical',\n",
       " 'character',\n",
       " 'recognition',\n",
       " '(',\n",
       " 'OCR',\n",
       " ')',\n",
       " 'it',\n",
       " 'gives',\n",
       " 'an',\n",
       " 'image',\n",
       " 'representing',\n",
       " 'printed',\n",
       " 'text',\n",
       " ',',\n",
       " 'which',\n",
       " 'help',\n",
       " 'in',\n",
       " 'determining',\n",
       " 'the',\n",
       " 'corresponding',\n",
       " 'or',\n",
       " 'related',\n",
       " 'text',\n",
       " '.',\n",
       " 'Part',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'tagging',\n",
       " 'it',\n",
       " 'describes',\n",
       " 'a',\n",
       " 'sentence',\n",
       " ',',\n",
       " 'determines',\n",
       " 'the',\n",
       " 'part',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'for',\n",
       " 'each',\n",
       " 'word',\n",
       " '.',\n",
       " 'Though',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'are',\n",
       " 'obviously',\n",
       " 'very',\n",
       " 'closely',\n",
       " 'interweaved',\n",
       " 'but',\n",
       " 'they',\n",
       " 'are',\n",
       " 'used',\n",
       " 'frequently',\n",
       " ',',\n",
       " 'for',\n",
       " 'convenience',\n",
       " '.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'task',\n",
       " 'such',\n",
       " 'as',\n",
       " 'automatic',\n",
       " 'summarisation',\n",
       " ',',\n",
       " 'co-reference',\n",
       " 'analysis',\n",
       " 'etc',\n",
       " '.',\n",
       " 'act',\n",
       " 'as',\n",
       " 'subtask',\n",
       " 'that',\n",
       " 'are',\n",
       " 'used',\n",
       " 'in',\n",
       " 'solving',\n",
       " 'larger',\n",
       " 'tasks']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9992469",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(token1 , tagset = \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b407e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Noah', 'NNP'),\n",
       " ('Chomsky', 'NNP'),\n",
       " (',', ','),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('linguists', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('twelfth', 'JJ'),\n",
       " ('century', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('started', 'VBD'),\n",
       " ('syntactic', 'JJ'),\n",
       " ('theories', 'NNS'),\n",
       " (',', ','),\n",
       " ('marked', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('unique', 'JJ'),\n",
       " ('position', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('field', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('theoretical', 'JJ'),\n",
       " ('linguistics', 'NNS'),\n",
       " ('because', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('revolutionised', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('area', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('syntax', 'NN'),\n",
       " ('(', '('),\n",
       " ('Chomsky', 'NNP'),\n",
       " (',', ','),\n",
       " ('1965', 'CD'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('Which', 'WDT'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('broadly', 'RB'),\n",
       " ('categorized', 'VBN'),\n",
       " ('into', 'IN'),\n",
       " ('two', 'CD'),\n",
       " ('levels', 'NNS'),\n",
       " ('Higher', 'JJR'),\n",
       " ('Level', 'NNP'),\n",
       " ('which', 'WDT'),\n",
       " ('include', 'VBP'),\n",
       " ('speech', 'JJ'),\n",
       " ('recognition', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('Lower', 'NNP'),\n",
       " ('Level', 'NNP'),\n",
       " ('which', 'WDT'),\n",
       " ('corresponds', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Few', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('researched', 'JJ'),\n",
       " ('tasks', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('Automatic', 'NNP'),\n",
       " ('Summarization', 'NNP'),\n",
       " (',', ','),\n",
       " ('Co-Reference', 'NNP'),\n",
       " ('Resolution', 'NNP'),\n",
       " (',', ','),\n",
       " ('Discourse', 'NNP'),\n",
       " ('Analysis', 'NNP'),\n",
       " (',', ','),\n",
       " ('Machine', 'NNP'),\n",
       " ('Translation', 'NNP'),\n",
       " (',', ','),\n",
       " ('Morphological', 'NNP'),\n",
       " ('Segmentation', 'NNP'),\n",
       " (',', ','),\n",
       " ('Named', 'NNP'),\n",
       " ('Entity', 'NNP'),\n",
       " ('Recognition', 'NNP'),\n",
       " (',', ','),\n",
       " ('Optical', 'NNP'),\n",
       " ('Character', 'NNP'),\n",
       " ('Recognition', 'NNP'),\n",
       " (',', ','),\n",
       " ('Part', 'NNP'),\n",
       " ('Of', 'IN'),\n",
       " ('Speech', 'NNP'),\n",
       " ('Tagging', 'NNP'),\n",
       " ('etc', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('tasks', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('direct', 'JJ'),\n",
       " ('real', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('applications', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('Machine', 'NNP'),\n",
       " ('translation', 'NN'),\n",
       " (',', ','),\n",
       " ('Named', 'NNP'),\n",
       " ('entity', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " (',', ','),\n",
       " ('Optical', 'NNP'),\n",
       " ('character', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " ('etc', 'FW'),\n",
       " ('.', '.'),\n",
       " ('Automatic', 'JJ'),\n",
       " ('summarization', 'NN'),\n",
       " ('produces', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('understandable', 'JJ'),\n",
       " ('summary', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('set', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('provides', 'VBZ'),\n",
       " ('summaries', 'NNS'),\n",
       " ('or', 'CC'),\n",
       " ('detailed', 'VBN'),\n",
       " ('information', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('known', 'VBN'),\n",
       " ('type', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Co-reference', 'JJ'),\n",
       " ('resolution', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('refers', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('larger', 'JJR'),\n",
       " ('set', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('determines', 'VBZ'),\n",
       " ('which', 'WDT'),\n",
       " ('word', 'NN'),\n",
       " ('refer', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('same', 'JJ'),\n",
       " ('object', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Discourse', 'NNP'),\n",
       " ('analysis', 'NN'),\n",
       " ('refers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('task', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('identifying', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('discourse', 'NN'),\n",
       " ('structure', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('connected', 'JJ'),\n",
       " ('text', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Machine', 'NNP'),\n",
       " ('translation', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('refers', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('automatic', 'JJ'),\n",
       " ('translation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('one', 'CD'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('another', 'DT'),\n",
       " ('.', '.'),\n",
       " ('Morphological', 'NNP'),\n",
       " ('segmentation', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('refers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('separate', 'VB'),\n",
       " ('word', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('individual', 'JJ'),\n",
       " ('morphemes', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('identify', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('class', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('morphemes', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Named', 'VBN'),\n",
       " ('entity', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " ('(', '('),\n",
       " ('NER', 'NNP'),\n",
       " (')', ')'),\n",
       " ('it', 'PRP'),\n",
       " ('describes', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('stream', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " (',', ','),\n",
       " ('determine', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('items', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('relates', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('proper', 'VB'),\n",
       " ('names', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Optical', 'JJ'),\n",
       " ('character', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " ('(', '('),\n",
       " ('OCR', 'NNP'),\n",
       " (')', ')'),\n",
       " ('it', 'PRP'),\n",
       " ('gives', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('image', 'NN'),\n",
       " ('representing', 'VBG'),\n",
       " ('printed', 'VBN'),\n",
       " ('text', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('help', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('determining', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('corresponding', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('related', 'VBN'),\n",
       " ('text', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('tagging', 'VBG'),\n",
       " ('it', 'PRP'),\n",
       " ('describes', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " (',', ','),\n",
       " ('determines', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('each', 'DT'),\n",
       " ('word', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Though', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('tasks', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('obviously', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('closely', 'RB'),\n",
       " ('interweaved', 'VBN'),\n",
       " ('but', 'CC'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('used', 'VBN'),\n",
       " ('frequently', 'RB'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('convenience', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Some', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('task', 'NN'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('automatic', 'JJ'),\n",
       " ('summarisation', 'NN'),\n",
       " (',', ','),\n",
       " ('co-reference', 'NN'),\n",
       " ('analysis', 'NN'),\n",
       " ('etc', 'NN'),\n",
       " ('.', '.'),\n",
       " ('act', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('subtask', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('used', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('solving', 'VBG'),\n",
       " ('larger', 'JJR'),\n",
       " ('tasks', 'NNS')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af215148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['started', 'marked', 'revolutionised', 'be', 'categorized', 'include', 'corresponds', 'are', 'have', 'produces', 'provides', 'detailed', 'known', 'refers', 'determines', 'identifying', 'refers', 'separate', 'identify', 'Named', 'describes', 'relates', 'proper', 'gives', 'representing']\n",
      "Adjectives: ['first', 'twelfth', 'syntactic', 'unique', 'theoretical', 'Higher', 'speech', 'natural', 'researched', 'direct', 'real', 'such', 'Automatic', 'understandable', 'Co-reference', 'larger', 'same', 'connected', 'automatic', 'human', 'individual', 'Optical', 'such', 'automatic', 'larger']\n",
      "Nouns: ['Noah', 'Chomsky', 'linguists', 'century', 'theories', 'position', 'field', 'linguistics', 'area', 'syntax', 'Chomsky', 'levels', 'Level', 'recognition', 'Lower', 'Level', 'language', 'tasks', 'NLP', 'Automatic', 'Summarization', 'Co-Reference', 'Resolution', 'Discourse', 'Analysis']\n"
     ]
    }
   ],
   "source": [
    "verbs = []\n",
    "adjectives = []\n",
    "nouns = []\n",
    "\n",
    "for word, pos in tags:\n",
    "    if pos.startswith('VB'):\n",
    "        verbs.append(word)\n",
    "    elif pos.startswith('JJ'):\n",
    "        adjectives.append(word)\n",
    "    elif pos.startswith('NN'):\n",
    "        nouns.append(word)\n",
    "\n",
    "print(\"Verbs:\", verbs[:25])\n",
    "print(\"Adjectives:\", adjectives[:25])\n",
    "print(\"Nouns:\", nouns[:25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a5402",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b71ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc66c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_verbs = [stem.stem(word) for word in verbs]\n",
    "stem_adjectives = [stem.stem(word) for word in adjectives]\n",
    "stem_nouns = [stem.stem(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b4538da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Verbs: ['start', 'mark', 'revolutionis', 'be', 'categor', 'includ', 'correspond', 'are', 'have', 'produc', 'provid', 'detail', 'known', 'refer', 'determin', 'identifi', 'refer', 'separ', 'identifi', 'name', 'describ', 'relat', 'proper', 'give', 'repres']\n",
      "Stemmed Adjectives: ['first', 'twelfth', 'syntact', 'uniqu', 'theoret', 'higher', 'speech', 'natur', 'research', 'direct', 'real', 'such', 'automat', 'understand', 'co-refer', 'larger', 'same', 'connect', 'automat', 'human', 'individu', 'optic', 'such', 'automat', 'larger']\n",
      "Stemmed Nouns: ['noah', 'chomski', 'linguist', 'centuri', 'theori', 'posit', 'field', 'linguist', 'area', 'syntax', 'chomski', 'level', 'level', 'recognit', 'lower', 'level', 'languag', 'task', 'nlp', 'automat', 'summar', 'co-refer', 'resolut', 'discours', 'analysi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Verbs:\", stem_verbs[:25])\n",
    "print(\"Stemmed Adjectives:\", stem_adjectives[:25])\n",
    "print(\"Stemmed Nouns:\", stem_nouns[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241aebe",
   "metadata": {},
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "295fd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stems = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d85d29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_verbss = [stems.stem(word) for word in verbs]\n",
    "stem_adjectivess = [stems.stem(word) for word in adjectives]\n",
    "stem_nounss = [stems.stem(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f79603f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Verbs: ['start', 'mark', 'revolutionis', 'be', 'categor', 'includ', 'correspond', 'are', 'have', 'produc', 'provid', 'detail', 'known', 'refer', 'determin', 'identifi', 'refer', 'separ', 'identifi', 'name', 'describ', 'relat', 'proper', 'give', 'repres']\n",
      "Stemmed Adjectives: ['first', 'twelfth', 'syntact', 'uniqu', 'theoret', 'higher', 'speech', 'natur', 'research', 'direct', 'real', 'such', 'automat', 'understand', 'co-refer', 'larger', 'same', 'connect', 'automat', 'human', 'individu', 'optic', 'such', 'automat', 'larger']\n",
      "Stemmed Nouns: ['noah', 'chomski', 'linguist', 'centuri', 'theori', 'posit', 'field', 'linguist', 'area', 'syntax', 'chomski', 'level', 'level', 'recognit', 'lower', 'level', 'languag', 'task', 'nlp', 'automat', 'summar', 'co-refer', 'resolut', 'discours', 'analysi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Verbs:\", stem_verbss[:25])\n",
    "print(\"Stemmed Adjectives:\", stem_adjectivess[:25])\n",
    "print(\"Stemmed Nouns:\", stem_nounss[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e5b9d",
   "metadata": {},
   "source": [
    "### Lancesterstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "324d6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "steml = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc58f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_verbsl= [steml.stem(word) for word in verbs]\n",
    "stem_adjectivesl= [steml.stem(word) for word in adjectives]\n",
    "stem_nounsl= [steml.stem(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6ab8c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Verbs: ['start', 'mark', 'revolv', 'be', 'categ', 'includ', 'correspond', 'ar', 'hav', 'produc', 'provid', 'detail', 'known', 'ref', 'determin', 'ident', 'ref', 'sep', 'ident', 'nam', 'describ', 'rel', 'prop', 'giv', 'repres']\n",
      "Stemmed Adjectives: ['first', 'twelf', 'syntact', 'un', 'theoret', 'high', 'speech', 'nat', 'research', 'direct', 'real', 'such', 'autom', 'understand', 'co-reference', 'larg', 'sam', 'connect', 'autom', 'hum', 'individ', 'opt', 'such', 'autom', 'larg']\n",
      "Stemmed Nouns: ['noah', 'chomsky', 'lingu', 'century', 'the', 'posit', 'field', 'lingu', 'are', 'syntax', 'chomsky', 'level', 'level', 'recognit', 'low', 'level', 'langu', 'task', 'nlp', 'autom', 'summ', 'co-reference', 'resolv', 'discours', 'analys']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Verbs:\", stem_verbsl[:25])\n",
    "print(\"Stemmed Adjectives:\", stem_adjectivesl[:25])\n",
    "print(\"Stemmed Nouns:\", stem_nounsl[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479de5f",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0406af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemr = RegexpStemmer('ing$|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fa5a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_verbsr= [stemr.stem(word) for word in verbs]\n",
    "stem_adjectivesr= [stemr.stem(word) for word in adjectives]\n",
    "stem_nounsr= [stemr.stem(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a30fdb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Verbs: ['started', 'marked', 'revolutionised', 'be', 'categorized', 'includ', 'correspond', 'are', 'hav', 'produce', 'provide', 'detailed', 'known', 'refer', 'determine', 'identify', 'refer', 'separat', 'identify', 'Named', 'describe', 'relate', 'proper', 'give', 'represent']\n",
      "Stemmed Adjectives: ['first', 'twelfth', 'syntactic', 'uniqu', 'theoretical', 'Higher', 'speech', 'natural', 'researched', 'direct', 'real', 'such', 'Automatic', 'understand', 'Co-referenc', 'larger', 'sam', 'connected', 'automatic', 'human', 'individual', 'Optical', 'such', 'automatic', 'larger']\n",
      "Stemmed Nouns: ['Noah', 'Chomsky', 'linguist', 'century', 'theorie', 'position', 'field', 'linguistic', 'area', 'syntax', 'Chomsky', 'level', 'Level', 'recognition', 'Lower', 'Level', 'languag', 'task', 'NLP', 'Automatic', 'Summarization', 'Co-Referenc', 'Resolution', 'Discours', 'Analysi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Verbs:\", stem_verbsr[:25])\n",
    "print(\"Stemmed Adjectives:\", stem_adjectivesr[:25])\n",
    "print(\"Stemmed Nouns:\", stem_nounsr[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ba54f",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd2ffe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import lemmatized_verbs\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87241fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_verbs = [lemmatizer.lemmatize(word, pos='v') for word in verbs]  #word = word to be lemmatized, pos tag\n",
    "lemmatized_adjectives = [lemmatizer.lemmatize(word, pos='a') for word in adjectives]\n",
    "lemmatized_nouns = [lemmatizer.lemmatize(word, pos='n') for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "508aac5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Verbs: ['start', 'mark', 'revolutionise', 'be', 'categorize', 'include', 'correspond', 'be', 'have', 'produce', 'provide', 'detail', 'know', 'refer', 'determine', 'identify', 'refer', 'separate', 'identify', 'Named', 'describe', 'relate', 'proper', 'give', 'represent']\n",
      "Lemmatized Adjectives: ['first', 'twelfth', 'syntactic', 'unique', 'theoretical', 'Higher', 'speech', 'natural', 'researched', 'direct', 'real', 'such', 'Automatic', 'understandable', 'Co-reference', 'large', 'same', 'connected', 'automatic', 'human', 'individual', 'Optical', 'such', 'automatic', 'large']\n",
      "Lemmatized Nouns: ['Noah', 'Chomsky', 'linguist', 'century', 'theory', 'position', 'field', 'linguistics', 'area', 'syntax', 'Chomsky', 'level', 'Level', 'recognition', 'Lower', 'Level', 'language', 'task', 'NLP', 'Automatic', 'Summarization', 'Co-Reference', 'Resolution', 'Discourse', 'Analysis']\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Verbs:\", lemmatized_verbs[:25])\n",
    "print(\"Lemmatized Adjectives:\", lemmatized_adjectives[:25])\n",
    "print(\"Lemmatized Nouns:\", lemmatized_nouns[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4f969bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verb</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>RegexpStemmer</th>\n",
       "      <th>lemmatized_verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>started</td>\n",
       "      <td>start</td>\n",
       "      <td>start</td>\n",
       "      <td>start</td>\n",
       "      <td>started</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marked</td>\n",
       "      <td>mark</td>\n",
       "      <td>mark</td>\n",
       "      <td>mark</td>\n",
       "      <td>marked</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revolutionised</td>\n",
       "      <td>revolutionis</td>\n",
       "      <td>revolutionis</td>\n",
       "      <td>revolv</td>\n",
       "      <td>revolutionised</td>\n",
       "      <td>revolutionise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>categorized</td>\n",
       "      <td>categor</td>\n",
       "      <td>categor</td>\n",
       "      <td>categ</td>\n",
       "      <td>categorized</td>\n",
       "      <td>categorize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>include</td>\n",
       "      <td>includ</td>\n",
       "      <td>includ</td>\n",
       "      <td>includ</td>\n",
       "      <td>includ</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>corresponds</td>\n",
       "      <td>correspond</td>\n",
       "      <td>correspond</td>\n",
       "      <td>correspond</td>\n",
       "      <td>correspond</td>\n",
       "      <td>correspond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>ar</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>hav</td>\n",
       "      <td>hav</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>produces</td>\n",
       "      <td>produc</td>\n",
       "      <td>produc</td>\n",
       "      <td>produc</td>\n",
       "      <td>produce</td>\n",
       "      <td>produce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>provides</td>\n",
       "      <td>provid</td>\n",
       "      <td>provid</td>\n",
       "      <td>provid</td>\n",
       "      <td>provide</td>\n",
       "      <td>provide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>detailed</td>\n",
       "      <td>detail</td>\n",
       "      <td>detail</td>\n",
       "      <td>detail</td>\n",
       "      <td>detailed</td>\n",
       "      <td>detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>known</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>refers</td>\n",
       "      <td>refer</td>\n",
       "      <td>refer</td>\n",
       "      <td>ref</td>\n",
       "      <td>refer</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>determines</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determine</td>\n",
       "      <td>determine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>identifying</td>\n",
       "      <td>identifi</td>\n",
       "      <td>identifi</td>\n",
       "      <td>ident</td>\n",
       "      <td>identify</td>\n",
       "      <td>identify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>refers</td>\n",
       "      <td>refer</td>\n",
       "      <td>refer</td>\n",
       "      <td>ref</td>\n",
       "      <td>refer</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>separate</td>\n",
       "      <td>separ</td>\n",
       "      <td>separ</td>\n",
       "      <td>sep</td>\n",
       "      <td>separat</td>\n",
       "      <td>separate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>identify</td>\n",
       "      <td>identifi</td>\n",
       "      <td>identifi</td>\n",
       "      <td>ident</td>\n",
       "      <td>identify</td>\n",
       "      <td>identify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Named</td>\n",
       "      <td>name</td>\n",
       "      <td>name</td>\n",
       "      <td>nam</td>\n",
       "      <td>Named</td>\n",
       "      <td>Named</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>describes</td>\n",
       "      <td>describ</td>\n",
       "      <td>describ</td>\n",
       "      <td>describ</td>\n",
       "      <td>describe</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relates</td>\n",
       "      <td>relat</td>\n",
       "      <td>relat</td>\n",
       "      <td>rel</td>\n",
       "      <td>relate</td>\n",
       "      <td>relate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>proper</td>\n",
       "      <td>proper</td>\n",
       "      <td>proper</td>\n",
       "      <td>prop</td>\n",
       "      <td>proper</td>\n",
       "      <td>proper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gives</td>\n",
       "      <td>give</td>\n",
       "      <td>give</td>\n",
       "      <td>giv</td>\n",
       "      <td>give</td>\n",
       "      <td>give</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>representing</td>\n",
       "      <td>repres</td>\n",
       "      <td>repres</td>\n",
       "      <td>repres</td>\n",
       "      <td>represent</td>\n",
       "      <td>represent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>printed</td>\n",
       "      <td>print</td>\n",
       "      <td>print</td>\n",
       "      <td>print</td>\n",
       "      <td>printed</td>\n",
       "      <td>print</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>determining</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>related</td>\n",
       "      <td>relat</td>\n",
       "      <td>relat</td>\n",
       "      <td>rel</td>\n",
       "      <td>related</td>\n",
       "      <td>relate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tagging</td>\n",
       "      <td>tag</td>\n",
       "      <td>tag</td>\n",
       "      <td>tag</td>\n",
       "      <td>tagg</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>describes</td>\n",
       "      <td>describ</td>\n",
       "      <td>describ</td>\n",
       "      <td>describ</td>\n",
       "      <td>describe</td>\n",
       "      <td>describe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>determines</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determin</td>\n",
       "      <td>determine</td>\n",
       "      <td>determine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>ar</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>interweaved</td>\n",
       "      <td>interweav</td>\n",
       "      <td>interweav</td>\n",
       "      <td>interweav</td>\n",
       "      <td>interweaved</td>\n",
       "      <td>interweave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>ar</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>used</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>us</td>\n",
       "      <td>used</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>ar</td>\n",
       "      <td>are</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>used</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>us</td>\n",
       "      <td>used</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>solving</td>\n",
       "      <td>solv</td>\n",
       "      <td>solv</td>\n",
       "      <td>solv</td>\n",
       "      <td>solv</td>\n",
       "      <td>solve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Verb PorterStemmer SnowballStemmer LancasterStemmer  \\\n",
       "0          started         start           start            start   \n",
       "1           marked          mark            mark             mark   \n",
       "2   revolutionised  revolutionis    revolutionis           revolv   \n",
       "3               be            be              be               be   \n",
       "4      categorized       categor         categor            categ   \n",
       "5          include        includ          includ           includ   \n",
       "6      corresponds    correspond      correspond       correspond   \n",
       "7              are           are             are               ar   \n",
       "8             have          have            have              hav   \n",
       "9         produces        produc          produc           produc   \n",
       "10        provides        provid          provid           provid   \n",
       "11        detailed        detail          detail           detail   \n",
       "12           known         known           known            known   \n",
       "13          refers         refer           refer              ref   \n",
       "14      determines      determin        determin         determin   \n",
       "15     identifying      identifi        identifi            ident   \n",
       "16          refers         refer           refer              ref   \n",
       "17        separate         separ           separ              sep   \n",
       "18        identify      identifi        identifi            ident   \n",
       "19           Named          name            name              nam   \n",
       "20       describes       describ         describ          describ   \n",
       "21         relates         relat           relat              rel   \n",
       "22          proper        proper          proper             prop   \n",
       "23           gives          give            give              giv   \n",
       "24    representing        repres          repres           repres   \n",
       "25         printed         print           print            print   \n",
       "26            help          help            help             help   \n",
       "27     determining      determin        determin         determin   \n",
       "28         related         relat           relat              rel   \n",
       "29         tagging           tag             tag              tag   \n",
       "30       describes       describ         describ          describ   \n",
       "31      determines      determin        determin         determin   \n",
       "32             are           are             are               ar   \n",
       "33     interweaved     interweav       interweav        interweav   \n",
       "34             are           are             are               ar   \n",
       "35            used           use             use               us   \n",
       "36             are           are             are               ar   \n",
       "37            used           use             use               us   \n",
       "38         solving          solv            solv             solv   \n",
       "\n",
       "     RegexpStemmer lemmatized_verbs  \n",
       "0          started            start  \n",
       "1           marked             mark  \n",
       "2   revolutionised    revolutionise  \n",
       "3               be               be  \n",
       "4      categorized       categorize  \n",
       "5           includ          include  \n",
       "6       correspond       correspond  \n",
       "7              are               be  \n",
       "8              hav             have  \n",
       "9          produce          produce  \n",
       "10         provide          provide  \n",
       "11        detailed           detail  \n",
       "12           known             know  \n",
       "13           refer            refer  \n",
       "14       determine        determine  \n",
       "15        identify         identify  \n",
       "16           refer            refer  \n",
       "17         separat         separate  \n",
       "18        identify         identify  \n",
       "19           Named            Named  \n",
       "20        describe         describe  \n",
       "21          relate           relate  \n",
       "22          proper           proper  \n",
       "23            give             give  \n",
       "24       represent        represent  \n",
       "25         printed            print  \n",
       "26            help             help  \n",
       "27        determin        determine  \n",
       "28         related           relate  \n",
       "29            tagg              tag  \n",
       "30        describe         describe  \n",
       "31       determine        determine  \n",
       "32             are               be  \n",
       "33     interweaved       interweave  \n",
       "34             are               be  \n",
       "35            used              use  \n",
       "36             are               be  \n",
       "37            used              use  \n",
       "38            solv            solve  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "v1= pd.DataFrame(verbs,columns=['Verb'])\n",
    "v2= pd.DataFrame(stem_verbs ,columns=['PorterStemmer'])\n",
    "v3= pd.DataFrame(stem_verbss,columns=['SnowballStemmer'])\n",
    "v4= pd.DataFrame(stem_verbsl,columns=['LancasterStemmer'])\n",
    "v5= pd.DataFrame(stem_verbsr,columns=['RegexpStemmer'])\n",
    "v6= pd.DataFrame(lemmatized_verbs,columns=['lemmatized_verbs'])\n",
    "result = pd.concat([v1, v2,v3,v4,v5,v6], axis=1, join='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f60e5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Noun</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>RegexpStemmer</th>\n",
       "      <th>lemmatized_verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Noah</td>\n",
       "      <td>noah</td>\n",
       "      <td>noah</td>\n",
       "      <td>noah</td>\n",
       "      <td>Noah</td>\n",
       "      <td>Noah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chomsky</td>\n",
       "      <td>chomski</td>\n",
       "      <td>chomski</td>\n",
       "      <td>chomsky</td>\n",
       "      <td>Chomsky</td>\n",
       "      <td>Chomsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linguists</td>\n",
       "      <td>linguist</td>\n",
       "      <td>linguist</td>\n",
       "      <td>lingu</td>\n",
       "      <td>linguist</td>\n",
       "      <td>linguist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>century</td>\n",
       "      <td>centuri</td>\n",
       "      <td>centuri</td>\n",
       "      <td>century</td>\n",
       "      <td>century</td>\n",
       "      <td>century</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theories</td>\n",
       "      <td>theori</td>\n",
       "      <td>theori</td>\n",
       "      <td>the</td>\n",
       "      <td>theorie</td>\n",
       "      <td>theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>analysis</td>\n",
       "      <td>analysi</td>\n",
       "      <td>analysi</td>\n",
       "      <td>analys</td>\n",
       "      <td>analysi</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "      <td>etc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>subtask</td>\n",
       "      <td>subtask</td>\n",
       "      <td>subtask</td>\n",
       "      <td>subtask</td>\n",
       "      <td>subtask</td>\n",
       "      <td>subtask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tasks</td>\n",
       "      <td>task</td>\n",
       "      <td>task</td>\n",
       "      <td>task</td>\n",
       "      <td>task</td>\n",
       "      <td>task</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Noun PorterStemmer SnowballStemmer LancasterStemmer RegexpStemmer  \\\n",
       "0         Noah          noah            noah             noah          Noah   \n",
       "1      Chomsky       chomski         chomski          chomsky       Chomsky   \n",
       "2    linguists      linguist        linguist            lingu      linguist   \n",
       "3      century       centuri         centuri          century       century   \n",
       "4     theories        theori          theori              the       theorie   \n",
       "..         ...           ...             ...              ...           ...   \n",
       "112   analysis       analysi         analysi           analys       analysi   \n",
       "113        etc           etc             etc              etc           etc   \n",
       "114        act           act             act              act           act   \n",
       "115    subtask       subtask         subtask          subtask       subtask   \n",
       "116      tasks          task            task             task          task   \n",
       "\n",
       "    lemmatized_verbs  \n",
       "0               Noah  \n",
       "1            Chomsky  \n",
       "2           linguist  \n",
       "3            century  \n",
       "4             theory  \n",
       "..               ...  \n",
       "112         analysis  \n",
       "113              etc  \n",
       "114              act  \n",
       "115          subtask  \n",
       "116             task  \n",
       "\n",
       "[117 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1= pd.DataFrame(nouns,columns=['Noun'])\n",
    "v2= pd.DataFrame(stem_nouns ,columns=['PorterStemmer'])\n",
    "v3= pd.DataFrame(stem_nounss,columns=['SnowballStemmer'])\n",
    "v4= pd.DataFrame(stem_nounsl,columns=['LancasterStemmer'])\n",
    "v5= pd.DataFrame(stem_nounsr,columns=['RegexpStemmer'])\n",
    "v6= pd.DataFrame(lemmatized_nouns,columns=['lemmatized_verbs'])\n",
    "result = pd.concat([v1, v2,v3,v4,v5,v6], axis=1, join='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c475395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjective</th>\n",
       "      <th>PorterStemmer</th>\n",
       "      <th>SnowballStemmer</th>\n",
       "      <th>LancasterStemmer</th>\n",
       "      <th>RegexpStemmer</th>\n",
       "      <th>lemmatized_verbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twelfth</td>\n",
       "      <td>twelfth</td>\n",
       "      <td>twelfth</td>\n",
       "      <td>twelf</td>\n",
       "      <td>twelfth</td>\n",
       "      <td>twelfth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>syntactic</td>\n",
       "      <td>syntact</td>\n",
       "      <td>syntact</td>\n",
       "      <td>syntact</td>\n",
       "      <td>syntactic</td>\n",
       "      <td>syntactic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique</td>\n",
       "      <td>uniqu</td>\n",
       "      <td>uniqu</td>\n",
       "      <td>un</td>\n",
       "      <td>uniqu</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theoretical</td>\n",
       "      <td>theoret</td>\n",
       "      <td>theoret</td>\n",
       "      <td>theoret</td>\n",
       "      <td>theoretical</td>\n",
       "      <td>theoretical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Higher</td>\n",
       "      <td>higher</td>\n",
       "      <td>higher</td>\n",
       "      <td>high</td>\n",
       "      <td>Higher</td>\n",
       "      <td>Higher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "      <td>speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>natural</td>\n",
       "      <td>natur</td>\n",
       "      <td>natur</td>\n",
       "      <td>nat</td>\n",
       "      <td>natural</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>researched</td>\n",
       "      <td>research</td>\n",
       "      <td>research</td>\n",
       "      <td>research</td>\n",
       "      <td>researched</td>\n",
       "      <td>researched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Automatic</td>\n",
       "      <td>automat</td>\n",
       "      <td>automat</td>\n",
       "      <td>autom</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>understandable</td>\n",
       "      <td>understand</td>\n",
       "      <td>understand</td>\n",
       "      <td>understand</td>\n",
       "      <td>understand</td>\n",
       "      <td>understandable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Co-reference</td>\n",
       "      <td>co-refer</td>\n",
       "      <td>co-refer</td>\n",
       "      <td>co-reference</td>\n",
       "      <td>Co-referenc</td>\n",
       "      <td>Co-reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>larger</td>\n",
       "      <td>larger</td>\n",
       "      <td>larger</td>\n",
       "      <td>larg</td>\n",
       "      <td>larger</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>same</td>\n",
       "      <td>same</td>\n",
       "      <td>same</td>\n",
       "      <td>sam</td>\n",
       "      <td>sam</td>\n",
       "      <td>same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "      <td>connected</td>\n",
       "      <td>connected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>automatic</td>\n",
       "      <td>automat</td>\n",
       "      <td>automat</td>\n",
       "      <td>autom</td>\n",
       "      <td>automatic</td>\n",
       "      <td>automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "      <td>hum</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>individual</td>\n",
       "      <td>individu</td>\n",
       "      <td>individu</td>\n",
       "      <td>individ</td>\n",
       "      <td>individual</td>\n",
       "      <td>individual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Optical</td>\n",
       "      <td>optic</td>\n",
       "      <td>optic</td>\n",
       "      <td>opt</td>\n",
       "      <td>Optical</td>\n",
       "      <td>Optical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "      <td>such</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>automatic</td>\n",
       "      <td>automat</td>\n",
       "      <td>automat</td>\n",
       "      <td>autom</td>\n",
       "      <td>automatic</td>\n",
       "      <td>automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>larger</td>\n",
       "      <td>larger</td>\n",
       "      <td>larger</td>\n",
       "      <td>larg</td>\n",
       "      <td>larger</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Adjective PorterStemmer SnowballStemmer LancasterStemmer  \\\n",
       "0            first         first           first            first   \n",
       "1          twelfth       twelfth         twelfth            twelf   \n",
       "2        syntactic       syntact         syntact          syntact   \n",
       "3           unique         uniqu           uniqu               un   \n",
       "4      theoretical       theoret         theoret          theoret   \n",
       "5           Higher        higher          higher             high   \n",
       "6           speech        speech          speech           speech   \n",
       "7          natural         natur           natur              nat   \n",
       "8       researched      research        research         research   \n",
       "9           direct        direct          direct           direct   \n",
       "10            real          real            real             real   \n",
       "11            such          such            such             such   \n",
       "12       Automatic       automat         automat            autom   \n",
       "13  understandable    understand      understand       understand   \n",
       "14    Co-reference      co-refer        co-refer     co-reference   \n",
       "15          larger        larger          larger             larg   \n",
       "16            same          same            same              sam   \n",
       "17       connected       connect         connect          connect   \n",
       "18       automatic       automat         automat            autom   \n",
       "19           human         human           human              hum   \n",
       "20      individual      individu        individu          individ   \n",
       "21         Optical         optic           optic              opt   \n",
       "22            such          such            such             such   \n",
       "23       automatic       automat         automat            autom   \n",
       "24          larger        larger          larger             larg   \n",
       "\n",
       "   RegexpStemmer lemmatized_verbs  \n",
       "0          first            first  \n",
       "1        twelfth          twelfth  \n",
       "2      syntactic        syntactic  \n",
       "3          uniqu           unique  \n",
       "4    theoretical      theoretical  \n",
       "5         Higher           Higher  \n",
       "6         speech           speech  \n",
       "7        natural          natural  \n",
       "8     researched       researched  \n",
       "9         direct           direct  \n",
       "10          real             real  \n",
       "11          such             such  \n",
       "12     Automatic        Automatic  \n",
       "13    understand   understandable  \n",
       "14   Co-referenc     Co-reference  \n",
       "15        larger            large  \n",
       "16           sam             same  \n",
       "17     connected        connected  \n",
       "18     automatic        automatic  \n",
       "19         human            human  \n",
       "20    individual       individual  \n",
       "21       Optical          Optical  \n",
       "22          such             such  \n",
       "23     automatic        automatic  \n",
       "24        larger            large  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1= pd.DataFrame(adjectives,columns=['Adjective'])\n",
    "v2= pd.DataFrame(stem_adjectives ,columns=['PorterStemmer'])\n",
    "v3= pd.DataFrame(stem_adjectivess,columns=['SnowballStemmer'])\n",
    "v4= pd.DataFrame(stem_adjectivesl,columns=['LancasterStemmer'])\n",
    "v5= pd.DataFrame(stem_adjectivesr,columns=['RegexpStemmer'])\n",
    "v6= pd.DataFrame(lemmatized_adjectives,columns=['lemmatized_verbs'])\n",
    "result = pd.concat([v1, v2,v3,v4,v5,v6], axis=1, join='inner')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc4bad",
   "metadata": {},
   "source": [
    "### 4(b) Write a program to perform Morphological generation on different surface forms of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8161526d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-statemachine\n",
      "  Downloading python_statemachine-2.0.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: python-statemachine\n",
      "Successfully installed python-statemachine-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-statemachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "528e15f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-statemachine in c:\\users\\acer\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-statemachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53163c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
